{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Py310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from scipy.ndimage import maximum_filter1d\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import noisereduce as nr\n",
    "import sounddevice as sd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.neural_network import MLPClassifier as MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_words = {0: \"Si\", 1: \"No\", 2: \"Agua\", 3: \"Comida\", 4: \"Dormir\"}\n",
    "experiments = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\"]\n",
    "start_times = [[1642014432149, 1642015279544, 1642018989749, 1642020887444],\n",
    "               [1642709615609, 1642711231738, 1642711835204, 1642712508208],\n",
    "               [1644518789205, 1644520973370, 1644522282435, 1644523772595],\n",
    "               [1644862510935, 1644863357729, 1644865356405, 1644866623555],\n",
    "               [1645482826638, 1645483436673, 1645484657373, 1645485205833],\n",
    "               [1645554802796, 1645555630886, 1645556859926, 1645558841261],\n",
    "               [1648486056563, 1648487671163, 1648488386468, 1648490709339],\n",
    "               [1649090396023, 1649091769558, 1649093282188, 1649094574062],\n",
    "               [1649278404169, 1649279608174, 1649281024999, 1649282379874],\n",
    "               [1650920673044, 1650923140274, 1650924725039, 1650926126008],\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the words identified by each code \n",
    "#and its start/finish time in ms from the .mrk file\n",
    "def get_durations(marks_path, palabras={2: \"Si\", 3: \"No\", 4: \"Agua\", 5: \"Comida\", 6: \"Dormir\"}):\n",
    "    durations = []\n",
    "\n",
    "    with open(marks_path) as file:\n",
    "        line_index = 0\n",
    "\n",
    "        for line in file:\n",
    "            marks = line[:line.rfind(\"\\n\")].split(\"\\t\")\n",
    "            stage = int(marks[0])\n",
    "            if stage != 1 and stage != 7:\n",
    "                durations.append([stage, palabras.get(stage), int(marks[1]), 0])\n",
    "            elif stage == 7:\n",
    "                durations[line_index][-1] = int(marks[1])\n",
    "                line_index += 1\n",
    "    \n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def for cutting words\n",
    "#Cuts from the signal passed the left and right parts of it\n",
    "#to_center = True, traverses the signal from the extremes to the center, stopping until the values found are >= max value of signal*percentage of each side\n",
    "#to_center = False, traverses the signal from the center outwards, stopping until the values found are <= max value of the signal*percentage of each side\n",
    "#If extra_ms_l/r is used, the cut word will get extra milliseconds before it starts or after it ends\n",
    "\n",
    "#Returns the index where the cutting of the signal started and a slice of the original signal, cut from the moment a disruption in the intensity\n",
    "#was found (depending on percentages given)\n",
    "\n",
    "def word_cut(data, freq, palabra, per_left=0.1, per_right=0.1, to_center=True, extra_ms_l=0, extra_ms_r=0):\n",
    "    ch0_o = data[palabra][2]\n",
    "    ch0 = maximum_filter1d(abs(ch0_o), size=1000)\n",
    "    max_abs_ch0 = np.argmax(ch0)\n",
    "\n",
    "    start_ch0, end_ch0 = 0, 0\n",
    "\n",
    "    if to_center:\n",
    "        for i in range(len(ch0)):\n",
    "            if abs(ch0[i]) >= ch0[max_abs_ch0]*per_left:\n",
    "                start_ch0 = i\n",
    "                break\n",
    "\n",
    "        for j in range(len(ch0)-1, 0, -1):\n",
    "            if abs(ch0[j]) >= ch0[max_abs_ch0]*per_right:\n",
    "                end_ch0 = j\n",
    "                break\n",
    "    else:\n",
    "        for i in range(max_abs_ch0, 0, -1):\n",
    "            if abs(ch0[i]) <= ch0[max_abs_ch0]*per_left:\n",
    "                start_ch0 = i\n",
    "                break\n",
    "\n",
    "        for j in range(max_abs_ch0, len(ch0)):\n",
    "            if abs(ch0[j]) <= ch0[max_abs_ch0]*per_right:\n",
    "                end_ch0 = j\n",
    "                break\n",
    "\n",
    "    extra_index = 0\n",
    "    if extra_ms_l > 0:\n",
    "        extra_index = int((extra_ms_l/1000.0)*freq)\n",
    "        start_ch0 -= extra_index\n",
    "\n",
    "        if start_ch0 < 0:\n",
    "            start_ch0 = 0\n",
    "    \n",
    "    if extra_ms_r > 0:\n",
    "        extra_index = int((extra_ms_r/1000.0)*freq)\n",
    "        end_ch0 += extra_index\n",
    "\n",
    "        if end_ch0 >= len(ch0_o):\n",
    "            end_ch0 = len(ch0_o)-1\n",
    "\n",
    "    word = np.array(ch0_o[start_ch0: end_ch0])\n",
    "    return start_ch0, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of lists of the words and its intensity data\n",
    "#Slices the data retrieved from the audio file\n",
    "#depending on the start/end times of the words in the durations array\n",
    "def get_word_stages(audio_data, fs, durations, start_time):\n",
    "    word_stages = []\n",
    "\n",
    "    for i in range(len(durations)):\n",
    "        start_cut = ((durations[i][2] - start_time)//1000)*fs\n",
    "        end_cut = ((durations[i][3] - start_time)//1000)*fs\n",
    "        word_stages.append(durations[i][:-2])\n",
    "        word_stages[i].append(audio_data[start_cut:end_cut])\n",
    "\n",
    "    return word_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives a list of lists which contains (each one) the cut words from all the wav files\n",
    "#and the indexes of when the word started to be pronounced\n",
    "def cut_all_words(data_dir, samples_array, all_samples=True, person_limit = 1, experiment_n=1):\n",
    "    sample = \"S\" + str(experiment_n)\n",
    "    limit = person_limit\n",
    "\n",
    "    if not 1 <= experiment_n <= len(samples_array):\n",
    "        sample = \"S1\"\n",
    "\n",
    "    if all_samples and not 1 <= limit <= len(samples_array):\n",
    "        limit = len(samples_array)\n",
    "    elif not all_samples:\n",
    "        limit = 1\n",
    "\n",
    "    cut_all_words = []\n",
    "    cutting_indexes = []\n",
    "    min_len = sys.maxsize\n",
    "    max_len = 0\n",
    "\n",
    "    for person in range(1, limit+1):\n",
    "        for wav in range(1, 5):\n",
    "            audio_dir = data_dir\n",
    "            marks_dir = data_dir\n",
    "            \n",
    "            if not all_samples:\n",
    "                audio_dir += sample + \"\\\\\" + sample + \"_\" + str(wav) + \".wav\"\n",
    "                marks_dir += sample + \"\\\\\" + sample + \"_HP_cruces\" + str(wav) + \".mrk\"\n",
    "            else:\n",
    "                audio_dir += \"S\" + str(person) + \"\\\\\" + \"S\" + str(person) + \"_\" + str(wav) + \".wav\"\n",
    "                marks_dir += \"S\" + str(person) + \"\\\\\" + \"S\" + str(person) + \"_HP_cruces\" + str(wav) + \".mrk\"\n",
    "\n",
    "            data, freq = librosa.load(audio_dir)\n",
    "            durations = get_durations(marks_dir)\n",
    "\n",
    "            if not all_samples:\n",
    "                words = get_word_stages(data, freq, durations, start_times[experiment_n-1][wav-1])\n",
    "            else:\n",
    "                words = get_word_stages(data, freq, durations, start_times[person-1][wav-1])\n",
    "\n",
    "            filtered_words = []\n",
    "            for tr in range(len(words)):\n",
    "                ff1 = nr.reduce_noise(words[tr][2], freq)\n",
    "                filtered_words.append(words[tr][:-1])\n",
    "                filtered_words[tr].append(ff1)\n",
    "\n",
    "            cut_words = []\n",
    "            current_cut_index = 0\n",
    "            for palabra in range(len(filtered_words)):\n",
    "                ind, cut_words_temp = word_cut(filtered_words, freq, palabra, per_right=0.01, to_center=False, extra_ms_r=100)\n",
    "                \n",
    "                if len(cut_words_temp) >= int(0.2*freq):\n",
    "                    cutting_indexes.append(ind)\n",
    "                    cut_words.append(filtered_words[palabra][:-1].copy())\n",
    "                    cut_words[current_cut_index].append(cut_words_temp)\n",
    "                    current_cut_index += 1\n",
    "                    \n",
    "                    if len(cut_words_temp) < min_len:\n",
    "                        min_len = len(cut_words_temp)\n",
    "\n",
    "                    if len(cut_words_temp) > max_len:\n",
    "                        max_len = len(cut_words_temp)\n",
    "\n",
    "            cut_all_words.append(cut_words)\n",
    "\n",
    "    return cut_all_words, cutting_indexes, min_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives the list of the cut words but alligned to a fixed len, so all the words will have the same size (used for getting the same number of mffc features)\n",
    "def get_alligned_words(cut_words, max_len):\n",
    "    alligned_cut_words = cut_words\n",
    "\n",
    "    for wav in range(len(alligned_cut_words)):\n",
    "        for word in range(len(alligned_cut_words[wav])):\n",
    "            temp_word = alligned_cut_words[wav][word][2].copy()\n",
    "            \n",
    "            if len(temp_word) > max_len:\n",
    "                alligned_cut_words[wav][word][2] = temp_word[:max_len]\n",
    "            else:\n",
    "                lack_len = max_len-len(temp_word)\n",
    "                expand = np.zeros(lack_len)\n",
    "                alligned_cut_words[wav][word][2] = np.concatenate((temp_word, expand))\n",
    "\n",
    "    return alligned_cut_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the mffc features of each of the cut words\n",
    "#Returns a list of lists containing the features of each individual word\n",
    "def get_words_mffc(cut_words_data, fr, put_targets=False):\n",
    "    all_mffcs = []\n",
    "\n",
    "    for record in range(len(cut_words_data)):\n",
    "        for word in range(len(cut_words_data[record])):\n",
    "            \n",
    "            signal = cut_words_data[record][word][2]\n",
    "            mffcs_features = librosa.feature.mfcc(signal, n_mfcc=13, sr=fr).flatten()\n",
    "            if not put_targets:\n",
    "                all_mffcs.append(mffcs_features)\n",
    "            else:\n",
    "                all_mffcs.append([cut_words_data[record][word][0], mffcs_features])\n",
    "    \n",
    "    return all_mffcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_array(cut_words_data):\n",
    "    training_array = []\n",
    "\n",
    "    for record in range(len(cut_words_data)):\n",
    "        for word in range(len(cut_words_data[record])):\n",
    "            \n",
    "            objective = cut_words_data[record][word][0]\n",
    "            training_array.append(objective)\n",
    "    \n",
    "    return training_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains and tests the selected model with the mffcs features from the wav audios and their targeted words\n",
    "#The model type can be any from the sklearn.svm.SVC types {'linear', 'poly', 'rbf', 'sigmoid'}. Default = \"linear\"\n",
    "#The number skf_n_folds is used to get the number of rounds for train and test indexes, commonly used 5 and 10. Default = 10\n",
    "\n",
    "#Prints the mean scores of the model: Accuaracy, Precision and Recall \n",
    "def get_model_scores(mffc_array, target_array, model_type=\"linear\", skf_n_folds=10):\n",
    "    clf = None\n",
    "    classifier_type = model_type\n",
    "\n",
    "    if model_type == \"tree\":\n",
    "        clf = DTC()\n",
    "    elif model_type == \"knn\":\n",
    "        clf = KNC()\n",
    "    elif model_type == \"neural_network\":\n",
    "        clf = MLPC()\n",
    "    else:\n",
    "        classifier_type = \"SVC - \" + model_type\n",
    "        clf = SVC(kernel = model_type)\n",
    "    \n",
    "    x = np.array(mffc_array)\n",
    "    y = np.array(target_array)\n",
    "    kf = StratifiedKFold(n_splits = skf_n_folds, shuffle = True)\n",
    "    \n",
    "    accuracy = 0\n",
    "    precision = np.zeros(5)\n",
    "    recall = np.zeros(5)\n",
    "\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "\n",
    "        # print(\"********************************\")\n",
    "\n",
    "        # Training phase\n",
    "        x_train = x[train_index]\n",
    "        y_train = y[train_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        # Test phase\n",
    "        x_test = x[test_index]\n",
    "        y_test = y[test_index]    \n",
    "        y_pred = clf.predict(x_test)    \n",
    "\n",
    "        accuracy_i = accuracy_score(y_test, y_pred)\n",
    "        accuracy += accuracy_i\n",
    "        # print('Accuracy:', accuracy_i)\n",
    "\n",
    "        precision_i = precision_score(y_test, y_pred, average = None)\n",
    "        precision += precision_i\n",
    "        # print('Precision:', precision_i)\n",
    "\n",
    "        recall_i = recall_score(y_test, y_pred, average = None)\n",
    "        recall += recall_i\n",
    "        # print('Recall:', recall_i)\n",
    "        \n",
    "    # print(\"********************************\")\n",
    "    print(f\"Mean scores for {classifier_type.upper()}\")\n",
    "    print(\"Accuracy:\", accuracy/skf_n_folds)\n",
    "    print(\"Mean Accuracy:\", sum(recall/skf_n_folds)/len(recall))\n",
    "    print(\"Precision:\", precision/skf_n_folds)\n",
    "    print(\"Recall:\", recall/skf_n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the average mffc of the desired word's mffcs\n",
    "#Change all_mffc to False if getting individual average mffc (1 experiment, according to exp_n) is desired\n",
    "\n",
    "#Returns a list of 5 lists, each containing the average mffc of each word\n",
    "def get_avg_mffc(data_path, samples_array, all_mffc=True, words_path=None, exp_n=1):\n",
    "    mffc_start_index = 0\n",
    "    data = None\n",
    "\n",
    "    if not all_mffc and not words_path == None:\n",
    "        words_file = open(words_path, \"rb\")\n",
    "        data = pickle.load(words_file)\n",
    "        words_file.close()\n",
    "\n",
    "        if not 1 <= exp_n <= len(samples_array):\n",
    "            print(\"The experiment number selected was outside the scope of the experiments done, number 1 was selected\\n\")\n",
    "            exp_n = 1\n",
    "        \n",
    "        for i in range(exp_n):\n",
    "            mffc_start_index += len(data[i])\n",
    "\n",
    "    elif not all_mffc and words_path == None:\n",
    "        print(\"Please enter a valid path for retrieving the alligned words array. This is necessary if you only want 1 experiment average's mffc\\n\")\n",
    "        return -1\n",
    "\n",
    "    features_file = open(data_path, \"rb\")\n",
    "    features = pickle.load(features_file)\n",
    "    features_file.close()\n",
    "\n",
    "    mffc_end_index = len(features)\n",
    "    if not all_mffc:\n",
    "        mffc_end_index = mffc_start_index + len(data[exp_n])\n",
    "\n",
    "    features_avg = [[],[],[],[],[]]\n",
    "    for word in range(mffc_start_index, mffc_end_index):\n",
    "        if features[word][0] == 2:\n",
    "            features_avg[0].append(features[word][1])\n",
    "        elif features[word][0] == 3:\n",
    "            features_avg[1].append(features[word][1])\n",
    "        elif features[word][0] == 4:\n",
    "            features_avg[2].append(features[word][1])\n",
    "        elif features[word][0] == 5:\n",
    "            features_avg[3].append(features[word][1])\n",
    "        else:\n",
    "            features_avg[4].append(features[word][1])\n",
    "\n",
    "    for word in range(len(features_avg)):\n",
    "        matrix_sum = []\n",
    "        \n",
    "        for mffc in range(len(features_avg[word])):\n",
    "            if mffc > 0:\n",
    "                matrix_sum += features_avg[word][mffc]\n",
    "            else:\n",
    "                matrix_sum = features_avg[word][mffc]\n",
    "        \n",
    "        matrix_sum /= len(features_avg[word])\n",
    "        features_avg[word] = matrix_sum\n",
    "    \n",
    "    return features_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots 5 spectograms (1 for each word) of the mffc features from the selected data file (needs to be an array with data for 5 spectograms)\n",
    "def plot_mffc_spectograms(path, fig_title, limit_colorbar=False):\n",
    "    file = open(path, \"rb\")\n",
    "    avg_mffc = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    if not type(avg_mffc) == list:\n",
    "        print(\"The input data file does not contain an object of type array. Needs to be a matrix of shape (5, 455)\")\n",
    "        return -1\n",
    "    if not len(avg_mffc) == 5:\n",
    "        print(\"The input data file does not contain a matrix of shape (5, 455)\")\n",
    "        return -1\n",
    "    if limit_colorbar:\n",
    "        min_value = np.amin(avg_mffc)\n",
    "        max_value = np.amax(avg_mffc)\n",
    "        norm = mpl.colors.Normalize(vmin=min_value, vmax=max_value)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(15, 30))\n",
    "    fig.set_facecolor(\"0.9\")\n",
    "    fig.suptitle(fig_title, fontsize=30)\n",
    "\n",
    "    subplots = fig.subplots(nrows=5, ncols=1)\n",
    "\n",
    "    img = []\n",
    "    for ax in range(len(subplots)):\n",
    "        word_features = np.reshape(avg_mffc[ax], (13, 35))\n",
    "        img.append(librosa.display.specshow(word_features, x_axis=\"time\", sr=22050, ax=subplots[ax]))\n",
    "        title = experiment_words.get(ax)\n",
    "        subplots[ax].set_title(title, fontsize=24)\n",
    "        subplots[ax].set_xlabel(\"\")\n",
    "        subplots[ax].set_xlabel(\"Time\", fontsize=15)\n",
    "        subplots[ax].set_ylabel(\"Features\", fontsize=15)\n",
    "\n",
    "    if limit_colorbar:\n",
    "        for im in img:\n",
    "            im.set_norm(norm)\n",
    "        for ax in range(len(subplots)):    \n",
    "            fig.colorbar(img[ax], ax=subplots[ax], norm=norm)\n",
    "    else:\n",
    "        fig.colorbar(img[ax], ax=subplots[ax])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Saving the alligned-cut words obtained from the .wav files to a file**\n",
    "\n",
    "### **Reading all the wav files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"D:\\\\Decodificación del habla\\\\words_cutWordsAllExp_data.obj\"\n",
    "read_all_files = True\n",
    "number_exp = len(experiments)\n",
    "max_allign = 0.8\n",
    "\n",
    "all_cut_words, indexes, min_len, max_len = cut_all_words(\"D:\\\\Decodificación del habla\\\\Datos\\\\\", experiments, all_samples=read_all_files, person_limit=number_exp)\n",
    "alligned_cut_words = get_alligned_words(all_cut_words.copy(), int(max_allign*22050))\n",
    "\n",
    "words_file = open(output_path, \"wb\")\n",
    "pickle.dump(all_cut_words, words_file)\n",
    "words_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading individual wav files**\n",
    "\n",
    "### **Change the `first_exp` and `last_exp` to select from which to which wav files to read**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_all_files = False\n",
    "max_allign = 0.8\n",
    "\n",
    "first_exp, last_exp = 6, 10\n",
    "for exp in range(first_exp, last_exp+1):\n",
    "    output_path = \"D:\\\\Decodificación del habla\\\\Datos de palabras\\\\words_cutWordsS\" + str(exp) + \"_data.obj\"\n",
    "\n",
    "    all_cut_words, indexes, min_len, max_len = cut_all_words(\"D:\\\\Decodificación del habla\\\\Datos\\\\\", experiments, all_samples=read_all_files, experiment_n=exp)\n",
    "    alligned_cut_words = get_alligned_words(all_cut_words.copy(), int(max_allign*22050))\n",
    "\n",
    "    words_file = open(output_path, \"wb\")\n",
    "    pickle.dump(all_cut_words, words_file)\n",
    "    words_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "EXAMPLE. Run to obtain the graph of a word (0-49, with some exceptions) from any of the wavs (0-39)\n",
    "\n",
    "- *wav_choosed* selects which from the 40 wavs you are going to select the word\n",
    "\n",
    "- *word_choosed* selects which from the 50 words is going to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_file = open(\"D:\\\\Decodificación del habla\\\\words_cutWordsAllExp_data.obj\", \"rb\")\n",
    "words_data = pickle.load(words_file)\n",
    "words_file.close()\n",
    "\n",
    "dt = 1/22050\n",
    "almost_max_len = 0\n",
    "wav_chooosed = 39\n",
    "word_choosed = 49\n",
    "\n",
    "choosed_word = words_data[wav_chooosed][word_choosed][2]\n",
    "t_size = len(choosed_word)\n",
    "\n",
    "x = np.arange(0, (t_size+1)*dt-dt, dt)\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "\n",
    "axis.plot(x, choosed_word)\n",
    "axis.set_title(f\"Wav {wav_chooosed+1}, palabra {word_choosed+1}\")\n",
    "axis.set_xlabel(\"Segundos\")\n",
    "axis.set_ylabel(\"Potencia\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE. Run to reproduce the word of the prior graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(words_data[wav_chooosed][word_choosed][2], 22050)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE. Run to obtain a Mel Spectogram of the first word of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal = words_data[0][0][2]\n",
    "test_mffcs = librosa.feature.mfcc(test_signal, n_mfcc=13, sr=22050)\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "librosa.display.specshow(test_mfccs, x_axis=\"time\", sr=22050)\n",
    "plt.colorbar(format=\"%+2f\")\n",
    "\n",
    "# print(test_mffcs)\n",
    "print(len(test_mffcs[0])*13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE. Run to get the first and second derivative and concatenate them in a single array\n",
    "\n",
    "_Derivatives can be used to get more features about a single audio/word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delta_mffcs = librosa.feature.delta(test_mfccs)\n",
    "test_delta2_mffcs = librosa.feature.delta(test_mfccs, order=2)\n",
    "\n",
    "comprehensive_mfccs = np.concatenate((test_mfccs, test_delta_mffcs, test_delta2_mffcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Retrieving and saving all the mffc features from the cut words and their respectively objectives to separate files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_path = \"D:\\\\Decodificación del habla\\\\\"\n",
    "exp_path = \"D:\\\\Decodificación del habla\\\\Datos de palabras\\\\\"\n",
    "first_exp, last_exp = 6, 10\n",
    "\n",
    "words_path = all_exp_path + \"words_cutWordsAllExp_data.obj\"\n",
    "mffc_path = all_exp_path + \"features_allMffcTargets_data.obj\"\n",
    "target_path = all_exp_path + \"target_allObjectives_data.obj\"\n",
    "\n",
    "all_file = open(words_path, \"rb\")\n",
    "data = pickle.load(all_file)\n",
    "all_file.close()\n",
    "\n",
    "mffc_features_words = get_words_mffc(data, 22050, put_targets=True)\n",
    "objectives_array = get_training_array(data)\n",
    "\n",
    "all_features_file = open(mffc_path, \"wb\")\n",
    "pickle.dump(mffc_features_words, all_features_file)\n",
    "all_features_file.close()\n",
    "\n",
    "all_targets_file = open(target_path, \"wb\")\n",
    "pickle.dump(objectives_array, all_targets_file)\n",
    "all_targets_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieving and saving the features and their respective targets of each individual experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in range(first_exp, last_exp+1):\n",
    "    words_path = exp_path + \"words_cutWordsS\" + str(exp) + \"_data.obj\"\n",
    "    mffc_path = exp_path + \"features_mffcS\" + str(exp) + \"_data.obj\"\n",
    "    target_path = exp_path + \"target_objectivesS\" + str(exp) + \"_data.obj\"\n",
    "\n",
    "    exp_file = open(words_path, \"rb\")\n",
    "    exp_data = pickle.load(exp_file)\n",
    "    exp_file.close()\n",
    "\n",
    "    mffc_features_words = get_words_mffc(exp_data, 22050)\n",
    "    objectives_array = get_training_array(exp_data)\n",
    "\n",
    "    features_file = open(mffc_path, \"wb\")\n",
    "    pickle.dump(mffc_features_words, features_file)\n",
    "    features_file.close()\n",
    "\n",
    "    training_array_file = open(target_path, \"wb\")\n",
    "    pickle.dump(objectives_array, training_array_file)\n",
    "    training_array_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Classification methods**\n",
    "\n",
    "#### Retrieving the data from each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Decodificación del habla\\\\Datos de palabras\\\\\"\n",
    "mffc_exp = []\n",
    "target_exp = []\n",
    "\n",
    "for exp in range(1, len(experiments)+1):\n",
    "    mffc_path = path + \"features_mffcS\" + str(exp) + \"_data.obj\"\n",
    "    target_path = path + \"target_objectivesS\" + str(exp) + \"_data.obj\"\n",
    "\n",
    "    features_file = open(mffc_path, \"rb\")\n",
    "    mffc_exp.append(pickle.load(features_file))\n",
    "    features_file.close()\n",
    "\n",
    "    training_array_file = open(target_path, \"rb\")\n",
    "    target_exp.append(pickle.load(training_array_file))\n",
    "    training_array_file.close()\n",
    "\n",
    "all_features_file = open(\"D:\\\\Decodificación del habla\\\\features_allMffc_data.obj\", \"rb\")\n",
    "all_mffc_features = pickle.load(all_features_file)\n",
    "all_features_file.close()\n",
    "\n",
    "all_targets_file = open(\"D:\\\\Decodificación del habla\\\\target_allObjectives_data.obj\", \"rb\")\n",
    "all_targets = pickle.load(all_targets_file)\n",
    "all_targets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing the classification models**\n",
    "\n",
    "#### Using 1994 samples (from all experiments) divided into 5 different categories/words:\n",
    "\n",
    "#### `{2: \"Si\", 3: \"No\", 4: \"Agua\", 5: \"Comida\", 6: \"Dormir\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.988964824120603\n",
      "Mean Accuracy: 0.9889615384615386\n",
      "Precision: [0.99756098 0.97803404 0.99262195 0.98547619 0.99262195]\n",
      "Recall: [0.995      0.9875     0.9925     0.9875     0.98230769]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9824497487437187\n",
      "Mean Accuracy: 0.9824102564102564\n",
      "Precision: [0.98786005 0.96413212 0.99756098 0.9899359  0.97565309]\n",
      "Recall: [0.9924359  0.98       0.9874359  0.97230769 0.97987179]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9388090452261306\n",
      "Mean Accuracy: 0.9387692307692307\n",
      "Precision: [0.97274078 0.92425936 0.92138635 0.9502586  0.93449352]\n",
      "Recall: [0.95730769 0.9475     0.95730769 0.91467949 0.91705128]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9909824120603015\n",
      "Mean Accuracy: 0.9909871794871794\n",
      "Precision: [1.         0.97345009 0.99756098 0.99256098 0.99279907]\n",
      "Recall: [0.995     0.995     0.985     0.99      0.9899359]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9884673366834171\n",
      "Mean Accuracy: 0.9884487179487179\n",
      "Precision: [0.99512195 0.97822009 0.99512195 0.98285692 0.9925    ]\n",
      "Recall: [0.9899359  0.99       0.9875     0.9924359  0.98237179]\n"
     ]
    }
   ],
   "source": [
    "get_model_scores(all_mffc_features, all_targets, model_type=\"linear\")\n",
    "print(\"\")\n",
    "# get_model_scores(all_mffc_features, all_targets, model_type=\"poly\")\n",
    "# print(\"\")\n",
    "get_model_scores(all_mffc_features, all_targets, model_type=\"rbf\")\n",
    "print(\"\")\n",
    "# get_model_scores(all_mffc_features, all_targets, model_type=\"sigmoid\")\n",
    "# print(\"\")\n",
    "get_model_scores(all_mffc_features, all_targets, model_type=\"tree\")\n",
    "print(\"\")\n",
    "get_model_scores(all_mffc_features, all_targets, model_type=\"knn\")\n",
    "print(\"\")\n",
    "get_model_scores(all_mffc_features, all_targets, model_type=\"neural_network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 200 samples (1 experiment) on average divided into 5 different categories/words:\n",
    "\n",
    "#### `{2: \"Si\", 3: \"No\", 4: \"Agua\", 5: \"Comida\", 6: \"Dormir\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------\n",
      "S1 - Person number 1\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.9949999999999999\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   1.   1.   0.98 1.  ]\n",
      "Recall: [0.975 1.    1.    1.    1.   ]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9800000000000001\n",
      "Mean Accuracy: 0.9800000000000001\n",
      "Precision: [1.         1.         1.         0.93333333 1.        ]\n",
      "Recall: [0.975 1.    1.    1.    0.925]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9800000000000001\n",
      "Mean Accuracy: 0.9800000000000001\n",
      "Precision: [1.   0.94 1.   1.   0.98]\n",
      "Recall: [1.    1.    0.975 1.    0.925]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9949999999999999\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   1.   1.   0.98 1.  ]\n",
      "Recall: [0.975 1.    1.    1.    1.   ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.985\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [1.   1.   0.96 0.98 1.  ]\n",
      "Recall: [1.    1.    0.975 0.975 0.975]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S2 - Person number 2\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.99\n",
      "Mean Accuracy: 0.99\n",
      "Precision: [0.98 1.   1.   1.   0.98]\n",
      "Recall: [1.   1.   1.   0.95 1.  ]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.99\n",
      "Mean Accuracy: 0.99\n",
      "Precision: [0.98 1.   1.   1.   0.98]\n",
      "Recall: [1.   1.   1.   0.95 1.  ]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.97\n",
      "Mean Accuracy: 0.97\n",
      "Precision: [0.98       0.98       1.         0.98       0.94666667]\n",
      "Recall: [1.    1.    0.975 0.875 1.   ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9949999999999999\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   0.98 1.   1.   1.  ]\n",
      "Recall: [1.    1.    1.    0.975 1.   ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.985\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [1.   1.   0.98 1.   0.96]\n",
      "Recall: [1.    0.975 1.    0.95  1.   ]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S3 - Person number 3\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9850000000000001\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [1.   1.   1.   0.98 0.96]\n",
      "Recall: [1.    0.975 1.    0.975 0.975]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9550000000000001\n",
      "Mean Accuracy: 0.9550000000000001\n",
      "Precision: [0.98  0.89  0.955 1.    1.   ]\n",
      "Recall: [0.95  0.95  0.9   0.975 1.   ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9949999999999999\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   1.   1.   1.   0.98]\n",
      "Recall: [1.    1.    1.    0.975 1.   ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9800000000000001\n",
      "Mean Accuracy: 0.9800000000000001\n",
      "Precision: [1.         1.         0.94666667 1.         0.98      ]\n",
      "Recall: [0.925 0.975 1.    1.    1.   ]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S4 - Person number 4\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.9894736842105264\n",
      "Mean Accuracy: 0.99\n",
      "Precision: [1.    0.98  1.    1.    0.975]\n",
      "Recall: [1.    0.975 1.    1.    0.975]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9850000000000001\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [0.98       0.96666667 1.         1.         1.        ]\n",
      "Recall: [0.975 0.975 1.    1.    0.975]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9539473684210525\n",
      "Mean Accuracy: 0.9516666666666665\n",
      "Precision: [0.98 0.92 0.98 1.   0.91]\n",
      "Recall: [0.975      0.925      0.975      0.975      0.90833333]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9947368421052631\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   0.98 1.   1.   1.  ]\n",
      "Recall: [1.    1.    1.    1.    0.975]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9747368421052632\n",
      "Mean Accuracy: 0.975\n",
      "Precision: [1.         0.94666667 1.         0.98       0.98      ]\n",
      "Recall: [0.975 0.975 1.    0.975 0.95 ]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S5 - Person number 5\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.99\n",
      "Mean Accuracy: 0.99\n",
      "Precision: [1.   0.96 1.   1.   1.  ]\n",
      "Recall: [1.   1.   0.95 1.   1.  ]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.96\n",
      "Mean Accuracy: 0.96\n",
      "Precision: [0.98       0.98       0.96       1.         0.93333333]\n",
      "Recall: [1.    0.975 0.9   0.975 0.95 ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.99\n",
      "Mean Accuracy: 0.99\n",
      "Precision: [1.   0.96 1.   1.   1.  ]\n",
      "Recall: [1.   1.   0.95 1.   1.  ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.985\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [0.98 0.98 1.   0.98 1.  ]\n",
      "Recall: [1.    1.    0.975 0.975 0.975]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S6 - Person number 6\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.985\n",
      "Mean Accuracy: 0.985\n",
      "Precision: [1.         1.         1.         0.96666667 0.98      ]\n",
      "Recall: [1.    0.975 1.    1.    0.95 ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9850000000000001\n",
      "Mean Accuracy: 0.9850000000000001\n",
      "Precision: [0.98 0.98 0.98 1.   1.  ]\n",
      "Recall: [0.975 1.    0.975 0.975 1.   ]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S7 - Person number 7\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.9897368421052631\n",
      "Mean Accuracy: 0.9883333333333333\n",
      "Precision: [1.   1.   0.98 0.98 1.  ]\n",
      "Recall: [1.         1.         0.975      1.         0.96666667]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.8944736842105264\n",
      "Mean Accuracy: 0.8933333333333333\n",
      "Precision: [1.         0.96666667 1.         0.70095238 0.96666667]\n",
      "Recall: [0.925      1.         0.975      1.         0.56666667]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.8292105263157895\n",
      "Mean Accuracy: 0.8283333333333334\n",
      "Precision: [0.82047619 0.935      0.94666667 0.79714286 0.72880952]\n",
      "Recall: [0.825      0.9        0.975      0.8        0.64166667]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.975\n",
      "Mean Accuracy: 0.975\n",
      "Precision: [1.         0.96666667 1.         0.94       1.        ]\n",
      "Recall: [0.95  1.    0.975 1.    0.95 ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.97\n",
      "Mean Accuracy: 0.97\n",
      "Precision: [1.   0.96 0.98 0.98 0.96]\n",
      "Recall: [0.925 0.975 0.975 1.    0.975]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S8 - Person number 8\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9642105263157894\n",
      "Mean Accuracy: 0.9649999999999999\n",
      "Precision: [0.98  0.98  0.98  1.    0.915]\n",
      "Recall: [1.    0.9   1.    0.95  0.975]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9897368421052631\n",
      "Mean Accuracy: 0.9883333333333333\n",
      "Precision: [0.98 0.98 1.   1.   1.  ]\n",
      "Recall: [1.         1.         0.975      0.96666667 1.        ]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S9 - Person number 9\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 0.9700000000000001\n",
      "Mean Accuracy: 0.9700000000000001\n",
      "Precision: [0.98 0.94 1.   0.96 1.  ]\n",
      "Recall: [1.    0.975 0.975 0.975 0.925]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9649999999999999\n",
      "Mean Accuracy: 0.9650000000000001\n",
      "Precision: [1.   0.9  0.96 1.   1.  ]\n",
      "Recall: [1.    0.95  0.975 0.975 0.925]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.9349999999999999\n",
      "Mean Accuracy: 0.9349999999999999\n",
      "Precision: [0.96       0.98       0.94       0.91333333 0.94      ]\n",
      "Recall: [0.975 0.925 0.975 0.9   0.9  ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.97\n",
      "Mean Accuracy: 0.9700000000000001\n",
      "Precision: [0.98       0.92666667 0.98       1.         1.        ]\n",
      "Recall: [1.    0.975 0.975 0.975 0.925]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.96\n",
      "Mean Accuracy: 0.96\n",
      "Precision: [1.         0.96       0.98       0.92166667 0.98      ]\n",
      "Recall: [0.925 0.975 1.    0.975 0.925]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "S10 - Person number 10\n",
      "\n",
      "Mean scores for SVC - LINEAR\n",
      "Accuracy: 1.0\n",
      "Mean Accuracy: 1.0\n",
      "Precision: [1. 1. 1. 1. 1.]\n",
      "Recall: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean scores for SVC - RBF\n",
      "Accuracy: 0.9650000000000001\n",
      "Mean Accuracy: 0.9650000000000001\n",
      "Precision: [0.98       0.94666667 0.98       0.96       1.        ]\n",
      "Recall: [0.975 1.    0.975 0.925 0.95 ]\n",
      "\n",
      "Mean scores for TREE\n",
      "Accuracy: 0.95\n",
      "Mean Accuracy: 0.95\n",
      "Precision: [0.955 0.975 0.94  0.96  0.96 ]\n",
      "Recall: [0.9   0.975 1.    0.925 0.95 ]\n",
      "\n",
      "Mean scores for KNN\n",
      "Accuracy: 0.9949999999999999\n",
      "Mean Accuracy: 0.9949999999999999\n",
      "Precision: [1.   0.98 1.   1.   1.  ]\n",
      "Recall: [1.    1.    1.    0.975 1.   ]\n",
      "\n",
      "Mean scores for NEURAL_NETWORK\n",
      "Accuracy: 0.9800000000000001\n",
      "Mean Accuracy: 0.9800000000000001\n",
      "Precision: [1.   0.96 1.   1.   0.96]\n",
      "Recall: [0.975 0.975 0.975 0.975 1.   ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp in range(len(mffc_exp)):\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"S{exp+1} - Person number {exp+1}\\n\")\n",
    "\n",
    "    get_model_scores(mffc_exp[exp], target_exp[exp], model_type=\"linear\")\n",
    "    print(\"\")\n",
    "    get_model_scores(mffc_exp[exp], target_exp[exp], model_type=\"rbf\")\n",
    "    print(\"\")\n",
    "    get_model_scores(mffc_exp[exp], target_exp[exp], model_type=\"tree\")\n",
    "    print(\"\")\n",
    "    get_model_scores(mffc_exp[exp], target_exp[exp], model_type=\"knn\")\n",
    "    print(\"\")\n",
    "    get_model_scores(mffc_exp[exp], target_exp[exp], model_type=\"neural_network\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Retrieving and saving the average mffc of each word _for all_ the experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Decodificación del habla\\\\features_allMffcTargets_data.obj\"\n",
    "words_path = \"D:\\\\Decodificación del habla\\\\words_cutWordsAllExp_Data.obj\"\n",
    "\n",
    "avg_mffc_all = get_avg_mffc(path, experiments)\n",
    "\n",
    "file = open(\"D:\\\\Decodificación del habla\\\\features_meanAllMffc_data.obj\", \"rb\")\n",
    "test = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieving and saving the average mffc of each word _for each_ experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Decodificación del habla\\\\features_allMffcTargets_data.obj\"\n",
    "words_path = \"D:\\\\Decodificación del habla\\\\words_cutWordsAllExp_Data.obj\"\n",
    "\n",
    "for exp in range(len(experiments)):\n",
    "    avg_mffc = get_avg_mffc(path, experiments, all_mffc=False, words_path=words_path, exp_n=exp+1)\n",
    "\n",
    "    save_file = \"D:\\\\Decodificación del habla\\\\Datos de palabras\\\\features_avgMffcS\" + str(exp+1) + \"_data.obj\"\n",
    "    file = open(save_file, \"wb\")\n",
    "    pickle.dump(avg_mffc, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plotting the average mffc _for all_ the experiments and _for each_ experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_path = \"D:\\\\Decodificación del habla\\\\features_meanAllMffc_data.obj\"\n",
    "ind_avg_path = \"D:\\\\Decodificación del habla\\\\Datos de palabras\\\\features_avgMffcS\"\n",
    "\n",
    "plot_mffc_spectograms(all_avg_path, \"Average mffc for all experiments\", limit_colorbar=True)\n",
    "\n",
    "for exp in range(len(experiments)):\n",
    "    path = ind_avg_path + str(exp+1) + \"_data.obj\"\n",
    "    title = \"Average mffc for exp S\" + str(exp+1)\n",
    "    plot_mffc_spectograms(path, title, limit_colorbar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e269d7884629e7b6fa4b46f6fe56f1cba66e1ba3a0b006ef510b9bb4313c3499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
